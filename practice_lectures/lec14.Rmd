---
title: "lec14.Rmd"
author: "Adam Kapelner"
date: "May 14, 2019"
output: html_document
---


# Missingness

Take a look at the weather dataset again:

```{r}
pacman::p_load(nycflights13, tidyverse, magrittr)
data(weather)
str(weather)
summary(weather)
```

Note how `time_hour` is perfectly collinear with the year, month, day and hour columns. So let's drop `time_hour`:

```{r}
weather %<>%
  select(-c(time_hour))
```

We also cannot have character variables:

```{r}
weather %<>%
  mutate(origin = as.factor(origin))
```


Imagine we were trying to predict `precip`. So let's section our dataset:

```{r}
y = weather$precip
X = weather
X$precip = NULL
rm(weather)
```

Let's first create a matrix with $p$ columns that represents missingness

```{r}
M = tbl_df(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
head(M)
summary(M)
```

Some of these missing indicators are collinear because they share all the rows they are missing on. Let's filter those out:

```{r}
M = tbl_df(t(unique(t(M))))
```


Many featuers did not have missingness so let's remove them:

```{r}
M %<>% select_if(function(x){sum(x) > 0})
head(M)
dim(M)
colSums(M)
```

Now let's work on imputation. The missingness is not extreme except in the `wind_gust` variable. Let's drop that variable:

```{r}
X %<>% select(-wind_gust)
```

Without imputing and without using missingness as a predictor in its own right, let's see what we get with a basic linear model now:

```{r}
lin_mod_listwise_deletion = lm(y ~ ., X)
summary(lin_mod_listwise_deletion)
```

A measly 13.2%. Also note: year is collinear.

Now let's impute using the package. we cannot fit RF models to the entire dataset (it's 26,000! observations) so we will sample 2,000 for each of the trees and then average. That will be good enough.

```{r}
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(2000, ncol(X)))$ximp
```

Now we take our imputed dataset, combine it with our missingness indicators for a new design matrix.

```{r}
Xnew = data.frame(cbind(Ximp, M))
linear_mod_impute_and_missing_dummies = lm(y ~ ., Xnew)
summary(linear_mod_impute_and_missing_dummies)
```

Now we have 13.5%. I bet that gain is statistically significant but it wasn't such a meaningful gain in real world terms. This is just an illustration of best practice. It didn't necessarily have to "work".

It is hard to compare the two models since the first model was built with 23,000 observations and this was built with the full 26,000 observations. Those extra 3,000 are the most difficult to predict on. This is complicated...



# Logistic Regression for Binary Response

Let's clean up and load the cancer dataset, remove missing data, remove the ID column and add more appropriate feature names:

```{r}
rm(list = ls())
biopsy = MASS::biopsy
biopsy$ID = NULL
biopsy = na.omit(biopsy)
colnames(biopsy) = c( #should've done this awhile ago!!!
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
```

Let's code "malignant" as 1 and "benign" as 0.

```{r}
biopsy$class = ifelse(biopsy$class == "malignant", 1, 0)
```


Now let's split into training and test for experiments:

```{r}
test_prop = 0.2
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```


Let's fit a linear logistic regression model. We use the function `glm` which looks a lot like `lm` except we have to pass in the parameter "binomial" which means we are using the independent Bernoulli. There are other types of models we won't get a chance to study e.g. Poisson, negative binomial.

```{r}
logistic_mod = glm(class ~ ., biopsy_train, family = "binomial")
```

Let's look at the $b$ vector that was made:

```{r}
coef(logistic_mod)
```

Interpretation? If clump thickness increases by one unit...

And let's take a look at the fitted values:

```{r}
head(predict(logistic_mod, biopsy_train))
```

What's that? Those are the "inverse link" values. In this case, they are log-odds of being malginant. If you can read log odds, you'll see ... has a small change of being malignant and ... has a high probability of being malignant. It's not that hard to read log odds...

What if we want probabilities? We can tell the predict function for `glm` to give us them explicitly:

```{r}
head(predict(logistic_mod, biopsy_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)))
```

It's very sure of itself! 

Let's see $y$ by $\hat{p}$ another way:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Made only a few mistakes here and there...

# Error metrics for Probabilistic Classification

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

This is very good Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)))
```

Still tends to be so sure of itself.

```{r}
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Not bad... and the Brier score?

```{r}
mean(-(y_test - p_hats_test)^2)
```

Not as good but still very good!

```{r}
rm(list = ls())
```

Let's try a harder project... load up the adult dataset where the response is 1 if the person makes more than \$50K per year and 0 if they make less than \$50K per year.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
str(adult)
?adult
```

Let's use samples of 5,000 to run experiments:

```{r}
train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Let's fit a logistic regression model to the training data:

```{r}
logistic_mod = glm(income ~ ., adult_train, family = "binomial")
```

Numeric errors already!

Let's see what the model looks like:

```{r}
coef(logistic_mod)
```

There may be NA's above due to numeric errors. Usually happens if there is linear dependence (or near linear dependence). Interpretation?

Let's take a look at the fitted probability estimates:

```{r}
head(predict(logistic_mod, adult_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, adult_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)))
```

Much more humble!! It's not a very confident model since this task is much harder! In fact it's never confident about the large incomes and usually confident about the small incomes.

Let's see $y$ by $\hat{p}$:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Making lots of mistakes!

Note that the x-axis is the native category label since we never coded as 0, 1. The default is that the first label is 0 and the second is 1. The labels are defaulted to alphabetical order (I think...)

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Can't use factors here. Need to code the response as 0/1

```{r}
table(as.numeric(y_train)) #casting works... almost...
y_train_numeric = as.numeric(y_train) - 1
mean(-(y_train_numeric - p_hats_train)^2)
```

This is worse than the previous dataset but not terrible. The null model gives what?

```{r}
mean(-(y_train_numeric - rep(mean(y_train_numeric), length(y_train_numeric)))^2)
```

So this is a decent Brier score! Again, most of the probabilities were spot on.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, adult_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)))
```

Looks similar to training. And the Brier score?

```{r}
y_test_numeric = as.numeric(y_test) - 1
mean(-(y_test_numeric - p_hats_test)^2)
```

The oos performance is about the same as the in-sample performance so we probably didn't overfit.

Brier scores only make sense if you know how to read Brier scores. It's kind of like learning a new language. However, everyon understands classification errors!


# Using Probability Estimation to do Classification

First repeat quickly (a) load the adult data (b) do a training / test split and (c) build the logisitc model.

```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness

train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL

logistic_mod = glm(income ~ ., adult_train, family = "binomial")
p_hats_train = predict(logistic_mod, adult_train, type = "response")
p_hats_test = predict(logistic_mod, adult_test, type = "response")
```

Let's establish a rule: if the probability estimate is greater than or equal to 50%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_train = factor(ifelse(p_hats_train >= 0.5, ">50K", "<=50K"))
```

How did this "classifier" do in-sample?

```{r}
mean(y_hats_train != y_train)
table(y_train, y_hats_train)
```

Let's see the same thing oos:

```{r}
y_hats_test = factor(ifelse(p_hats_test >= 0.5, ">50K", "<=50K"))
mean(y_hats_test != y_test)
oos_conf_table = table(y_test, y_hats_test)
oos_conf_table
```

A tad bit worse. Here are estimates of the future performance for each class:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

This whole classifier hinged on the decision of 50%! What if we change it??

# Asymmetric Cost Classifiers

Let's establish a *new* rule: if the probability estimate is greater than or equal to 90%, let's classify the observation as positive, otherwise 0.

```{r}
y_hats_train = factor(ifelse(p_hats_train >= 0.9, ">50K", "<=50K"))
mean(y_hats_train != y_train)
oos_conf_table = table(y_train, y_hats_train)
oos_conf_table
```

Of course the misclassification error went up! But now look at the confusion table! The second column represents all $\hat{y} = 1$ and there's not too many of them! Why? You've made it *much* harder to classify something as positive. Here's the new additional performance metrics now:

```{r}
n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos
cat("precision", round(precision * 100, 2), "%\n")
recall = tp / num_pos
cat("recall", round(recall * 100, 2), "%\n")
false_discovery_rate = 1 - precision
cat("false_discovery_rate", round(false_discovery_rate * 100, 2), "%\n")
false_omission_rate = fn / num_pred_neg
cat("false_omission_rate", round(false_omission_rate * 100, 2), "%\n")
```

We don't make many false discoveries but we make a lot of false omissions! It's a tradeoff...

# Receiver-Operator Curve Plot

The entire classifier is indexed by that indicator function probability threshold which creates the classification decision. Why not see look at the entire range of possible classification models. We do this with a function. We will go through it slowly and explain each piece:

```{r}
#' Computes performance metrics for a binary probabilistic classifer
#'
#' Each row of the result will represent one of the many models and its elements record the performance of that model so we can (1) pick a "best" model at the end and (2) overall understand the performance of the probability estimates a la the Brier scores, etc.
#'
#' @param p_hats  The probability estimates for n predictions
#' @param y_true  The true observed responses
#' @param res     The resolution to use for the grid of threshold values (defaults to 1e-3)
#'
#' @return        The matrix of all performance results
compute_metrics_prob_classifier = function(p_hats, y_true, res = 0.001){
  #we first make the grid of all prob thresholds
  p_thresholds = seq(0 + res, 1 - res, by = res) #values of 0 or 1 are trivial
  
  #now we create a matrix which will house all of our results
  performance_metrics = matrix(NA, nrow = length(p_thresholds), ncol = 12)
  colnames(performance_metrics) = c(
    "p_th",
    "TN",
    "FP",
    "FN",
    "TP",
    "miscl_err",
    "precision",
    "recall",
    "FDR",
    "FPR",
    "FOR",
    "miss_rate"
  )
  
  #now we iterate through each p_th and calculate all metrics about the classifier and save
  n = length(y_true)
  for (i in 1 : length(p_thresholds)){
    p_th = p_thresholds[i]
    y_hats = factor(ifelse(p_hats >= p_th, ">50K", "<=50K"))
    confusion_table = table(
      factor(y_true, levels = c("<=50K", ">50K")),
      factor(y_hats, levels = c("<=50K", ">50K"))
    )
      
    fp = confusion_table[1, 2]
    fn = confusion_table[2, 1]
    tp = confusion_table[2, 2]
    tn = confusion_table[1, 1]
    npp = sum(confusion_table[, 2])
    npn = sum(confusion_table[, 1])
    np = sum(confusion_table[2, ])
    nn = sum(confusion_table[1, ])
  
    performance_metrics[i, ] = c(
      p_th,
      tn,
      fp,
      fn,
      tp,
      (fp + fn) / n,
      tp / npp, #precision
      tp / np,  #recall
      fp / npp, #false discovery rate (FDR)
      fp / nn,  #false positive rate (FPR)
      fn / npn, #false omission rate (FOR)
      fn / np   #miss rate
    )
  }
  
  #finally return the matrix
  performance_metrics
}
```

Now let's generate performance results for the in-sample data:

```{r}
performance_metrics_in_sample = compute_metrics_prob_classifier(p_hats_train, y_train)

round(head(performance_metrics_in_sample), 3)
round(tail(performance_metrics_in_sample), 3)
```

Now let's plot the ROC curve

```{r}
pacman::p_load(ggplot2)
ggplot(data.frame(performance_metrics_in_sample)) +
  geom_line(aes(x = FPR, y = recall)) +
  geom_abline(intercept = 0, slope = 1, col = "red") + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1)
```

Now calculate the area under the curve (AUC) which is used to evaluate the probabilistic classifier (just like the Brier score) using a trapezoid area function. 

```{r}
pacman::p_load(pracma)
-trapz(performance_metrics_in_sample[, "FPR"], performance_metrics_in_sample[, "recall"])
```

This is not bad at all!

Note that I should add $<0, 0>$ and $<1, 1>$ as points before this is done but I didn't...

How do we do out of sample?


```{r}
performance_metrics_oos = compute_metrics_prob_classifier(p_hats_test, y_test)
```

And graph the ROC:


```{r}
ggplot(data.frame(performance_metrics_in_sample)) +
  geom_line(aes(x = FPR, y = recall)) +
  geom_abline(intercept = 0, slope = 1, col = "red") + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1) +
  geom_line(data = data.frame(performance_metrics_oos), aes(x = FPR, y = recall), col = "blue")
```


```{r}
-trapz(performance_metrics_oos[, "FPR"], performance_metrics_oos[, "recall"])
```


Not bad at all - only a tad worse! In the real world it's usually a lot worse. We are lucky we have 5,000 train and test.

# Detection Error Tradeoff curve

```{r}
ggplot(data.frame(performance_metrics_in_sample)) +
  geom_line(aes(x = FDR, y = miss_rate)) +
  coord_fixed() + xlim(0, 1) + ylim(0, 1) +
  geom_line(data = data.frame(performance_metrics_oos), aes(x = FDR, y = miss_rate), col = "blue")
```


#Using AUC to Compare Probabilistic Classification Models

What would the effect be of less information on the same traing set size? Imagine we didn't know the features: occupation, education, education_num, relationship, marital_status. How would we do relative to the above? Worse!

```{r}
logistic_mod_less_data = glm(income ~ . - occupation - education - education_num - relationship - marital_status, adult_train, family = "binomial")
p_hats_test = predict(logistic_mod_less_data, adult_test, type = "response")
performance_metrics_oos_mod_2 = compute_metrics_prob_classifier(p_hats_test, y_test)
ggplot(data.frame(performance_metrics_oos)) +
  geom_line(aes(x = FPR, y = recall), col = "blue") +
  geom_abline(intercept = 0, slope = 1, col = "red") + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1) +
  geom_line(data = data.frame(performance_metrics_oos_mod_2), aes(x = FPR, y = recall), col = "green")
```

and we can see clearly that the AUC is worse:

```{r}
-trapz(performance_metrics_oos_mod_2[, "FPR"], performance_metrics_oos_mod_2[, "recall"])
```

As we lose information that is related to the true causal inputs, we lose predictive ability. Same story for the entire class since error due to ignorance increases! And certainly no different in probabilistic classifiers.

# Choosing a Decision Threshold Based on Asymmetric Costs and Rewards

The ROC and DET curves gave you a glimpse into all the possibilities. Each point on that curve is a separate $g(x)$ with its own performance metrics. How do you pick one?

Let's create rewards and costs. Imagine we are trying to predict income because we want to sell people an expensive item e.g. a car. We want to advertise our cars via a nice packet in the mail. The packet costs \$5. If we send a packet to someone who really does make $>50K$/yr then we are expected to make \$1000. So we have rewards and costs below:

```{r}
r_tp = 1000 - 5
c_fp = -5
c_fn = -1000
r_tn = 0
```

Let's return to the linear logistic model with all features. Let's calculate the overall oos average reward per observation (per person) for each possible $p_{th}$:

```{r}
n = nrow(adult_test)
performance_metrics_oos = data.frame(performance_metrics_oos)
performance_metrics_oos$avg_reward = 
  (r_tp * performance_metrics_oos$TP +
  c_fp * performance_metrics_oos$FP +
  c_fn * performance_metrics_oos$FN +
  r_tn * performance_metrics_oos$TN) / n
```

Let's plot average reward (reward per person) by threshold:

```{r}
ggplot(performance_metrics_oos) +
  geom_line(aes(x = p_th, y = avg_reward)) + 
  geom_abline(intercept = 0, col = "red")
```

Obviously, the best decision is $p_{th} = 0$ which means you classifiy everything as a positive. This makes sense because the mailing is so cheap. The more interesting problem is where the cost of advertising is higher:

```{r}
r_tp = 1000 - 200
c_fp = -200
c_fn = -1000
r_tn = 0
performance_metrics_oos = data.frame(performance_metrics_oos)
performance_metrics_oos$avg_reward = 
  (r_tp * performance_metrics_oos$TP +
  c_fp * performance_metrics_oos$FP +
  c_fn * performance_metrics_oos$FN +
  r_tn * performance_metrics_oos$TN) / n
ggplot(performance_metrics_oos) +
  geom_point(aes(x = p_th, y = avg_reward), lwd = 0.01) + 
  geom_abline(intercept = 0, col = "red")
```

What are the performance characteristics of the optimal model?

```{r}
i_star = which.max(performance_metrics_oos$avg_reward)
round(as.matrix(performance_metrics_oos[i_star, ]), 2)
```

If $g_{pr}$ is closer to $f_{pr}$, what happens? All the threshold-derived classification models get better and you are guaranteed to make more money since you have a better discriminating eye.

