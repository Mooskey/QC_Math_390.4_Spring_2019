---
title: "Practice Lecture 9 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 9, 2019"
---


# Functions of y

In the diamonds dataset, the natural response is price:

```{r}
pacman::p_load(ggplot2)
data(diamonds)
ggplot(diamonds) + geom_histogram(aes(price), binwidth = 200)
mean(diamonds$price)
sd(diamonds$price)
```

Look at the long tail here. Popular wisdom says to log this type of distribution as a log transform on the y variable would possible make the model more linear in x. It would be easier to catch the long tail. This is "craft lore" or a "trick of the trade". Let's take a look at the distributiona after logging:

```{r}
ggplot(diamonds) + geom_histogram(aes(log(price)), binwidth = 0.03)
```

Some strange artifacts appear. Why the gap? Why is it "cut" at a maximum. These are questions to ask the one who collected the data.

Let's see if we get anywhere with this:

```{r}
lm_y = lm(price ~ ., diamonds)
lm_ln_y = lm(log(price) ~ ., diamonds)
summary(lm_y)$r.squared
summary(lm_ln_y)$r.squared
summary(lm_y)$sigma
summary(lm_ln_y)$sigma
``` 

Be careful when you use $g$ after logging, you will have to exponentiate the result. This is known to create bias because $E[Y]$ is different from $exp(E[ln(y)])$, but don't worry too much about this.

If you like this stuff, there are a whole bunch of transformations out there that are even cooler than the natural log. Some of this may be covered in 369 / 633. Let us use the log going forward:

```{r}
diamonds %<>%
  mutate(price = log(price))
```

# Cross Validation

This code gets the indices in a clever way:

```{r}
K = 5

all_idx = 1 : nrow(diamonds)
temp = rnorm(nrow(diamonds))
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = T, labels = F)
```


Now let's fit a model on this $K$ times:

```{r}
s_e_s = array(NA, K)
for (k in 1 : K){
  test_idx = all_idx[folds_vec == k]
  train_idx = setdiff(all_idx, test_idx)
  #sort(c(train_idx, test_idx))
  mod = lm(price ~ ., diamonds[train_idx, ])
  y_test = predict(mod, diamonds[test_idx, ])
  y = diamonds[test_idx, ]$price
  s_e_s[k] = sqrt(sum((y - y_test)^2)) / (length(test_idx))
}
s_e_s
```

# MLR library

```{r}
pacman::p_load(mlr)
```





# Model Selection

We have now covered non-linearities (e.g. polynomial terms) and interactions. A new complication now clearly emerges. If I have $p$ predictors, there are many linear least squares models I can build (considering non-linear least squares models makes the space of models even larger!!)

For instance, here are a bunch of models:

```{r}
mod1 = lm(price ~ carat + depth, diamonds) #using a subset of the features
mod2 = lm(price ~ ., diamonds) #using a subset of the features
mod3 = lm(price ~ poly(carat, 2) + poly(depth, 2), diamonds) #using some polynomial terms
mod4 = lm(price ~ . * ., diamonds) #using all interactions
coef(mod1)
coef(mod2)
coef(mod3)
coef(mod4)
```

Which model is "best"? 

This is one of the most fundamental problems in statistics, and possibly all of science! 

In class, we discussed validation via dividing $\mathbb{D}$ into (a) a training set and a (b) testing set. Now, we will further divide the training set into (a) a sub-training set and a (b) selection set and we still have the (c) test set. 

The total training set together will fit a model and testing will estimate future performance. But within the total training set, we'll use an elaborate algorithim: we'll fit many models and take the best one. That's the "master algorithm".

We'll make the selection set and the test set the same size but we don't have to. First split up the data:

```{r}
n = nrow(diamonds)
K = 5
test_indices = sample(1 : n, size = n * 1 / K)
master_train_indices = setdiff(1 : n, test_indices)
select_indices = sample(master_train_indices, size = n * 1 / K)
train_indices = setdiff(master_train_indices, select_indices)
rm(master_train_indices)

#make sure we did this right:
pacman::p_load(testthat)
expect_equal(1 : n, sort(c(train_indices, select_indices, test_indices)))

diamonds_train = diamonds[train_indices, ]
diamonds_select = diamonds[select_indices, ]
diamonds_test = diamonds[test_indices, ]

rm(test_indices, select_indices, train_indices)
```

Now, fit all models and select the best one:

```{r}
mod1 = lm(price ~ carat + depth, diamonds_train) #using a subset of the features
mod2 = lm(price ~ ., diamonds_train) #using a subset of the features
mod3 = lm(price ~ poly(carat, 2) + poly(depth, 2), diamonds_train) #using some polynomial terms
mod4 = lm(price ~ . * ., diamonds_train) #using all interactions
```

Now predict on the selection set and look at the oos $s_e$, and select the "best" model

```{r}
yhat_select_mod1 = predict(mod1, diamonds_select)
yhat_select_mod2 = predict(mod2, diamonds_select)
yhat_select_mod3 = predict(mod3, diamonds_select)
yhat_select_mod4 = predict(mod4, diamonds_select)
y_select = diamonds_select$price #the true prices

s_e_s = c(
  sd(yhat_select_mod1 - y_select), 
  sd(yhat_select_mod2 - y_select), 
  sd(yhat_select_mod3 - y_select), 
  sd(yhat_select_mod4 - y_select)
)
names(s_e_s) = paste("mod", 1 : 4, sep = "")
s_e_s
#find the minimum
names(which.min(s_e_s))
```

Which are overfit? Which are underfit? Were these models "poor choices"?

Can we go back and fit some more models? 

Yes - as long as we don't open the "lockbox" of the test set. Let's look at one more model. An expansion of the best of the previous 4 models now with a couple interactions we are convinced are real plus a couple of non-linear terms:

```{r}
mod5 = lm(price ~ 
            . + 
            carat * color + 
            carat * depth + 
            I(carat^2) +
            I(depth^2),
          diamonds_train) 

yhat_select_mod5 = predict(mod5, diamonds_select)

s_e_s = c(s_e_s, sd(yhat_select_mod5 - y_select))
names(s_e_s)[5] = "mod5"
s_e_s
#find the minimum
names(which.min(s_e_s))
```

We can go further and fit more and more models but we should always be careful that we don't fit too many as we may optimize to the selection set. Here, we are lucky since the selection set is large (~11,000 observations) so this is not too much of a fear.

If we had more time in class, we would investigate how we can build better models by looking at the regressions controlled for all other variables. It is an art! 

But you can see the problem - how can we build a good model??

The answer to this is non-parametric regression. But first, we will cover two other important topics before we get there.

Let us return and complete the exercise by now declaring we are done modeling and we are going to ship model 5. Let us get a conservative estimate of its performance:

```{r}
yhat_test_mod5 = predict(mod5, diamonds_test)
y_test = diamonds_test$price #the true prices
sd(yhat_test_mod5 - y_test)
```

About the same as the selection estimate --- we did not overfit too much to the selection set.

At this point the lockbox is open and we can never return (if we are honest, of course - many people in this business lie so beware).

Now we can build production model 5 with all data to ship:

```{r}
mod_final = lm(price ~ 
            . + 
            carat * color + 
            carat * depth + 
            I(carat^2) +
            I(depth^2),
          diamonds)
```

No evaluation is done on the final model. It is only used to predict future diamonds' prices.


# Forward Stepwise Linear Model Construction

Let's look at the diamonds data

```{r}
pacman::p_load(ggplot2)
data(diamonds)
diamonds$cut = factor(as.character(diamonds$cut))
diamonds$color = factor(as.character(diamonds$color))
diamonds$clarity = factor(as.character(diamonds$clarity))
```

What we're doing will be highly computational, so let's take a random sample of the dimaonds in $\mathbb{D}$:

```{r}
Nsamp = 1300
train_indices = sample(1 : nrow(diamonds), Nsamp)
diamonds_train = diamonds[train_indices, ]
```



Let's built a model with all second-order interactions

```{r}
mod = lm(price ~ . * . * ., diamonds_train)
```

How many variables is this? And what does it look like?

```{r}
length(coef(mod))
coef(mod)[1000 : 1100]
```

Remember we overfit just using first order interactions? We'll certainly overfit using first-order interactions AND second order interactions

```{r}
summary(mod)$r.squared
sd(summary(mod)$residuals)
```

Is that believable? Well... let's try it on the another 10,000 we didn't see...

```{r}
test_indices = sample(setdiff(1 : nrow(diamonds), train_indices), Nsamp)
diamonds_test = diamonds[test_indices, ]
y_hat_test = predict(mod, diamonds_test)
y_test = diamonds_test$price
e_test = y_test - y_hat_test
1 - sum((e_test)^2) / sum((y_test - mean(y_test))^2)
sd(e_test)
```

VERY negative oos $R^2$ --- why? What should that say about the relationship between $s_e$ and $s_y$?

```{r}
sd(y_test)
sd(e_test) / sd(y_test)
```

This is not only "overfitting"; it is an absolute trainwreck if you can do better using the null model (average of y) instead of your model.

So let us employ stepwise to get a good model. We need predictors to start with. How about `. * . * .` --- there's nothing intrinsically wrong with this. Let's create the model matrix:

```{r}
Xmm_train = model.matrix(price ~ . * . * ., diamonds_train)
y_train = diamonds_train$price
p_plus_one = ncol(Xmm_train)

Xmm_test = model.matrix(price ~ . * . * ., diamonds_test)
```

Now let's go through one by one and add the best one based on $s_e$ gain i.e. the best new dimension to add to project the most of the vector $y$ as possible onto the column space.

```{r}
predictor_by_iteration = c() #keep a growing list of predictors by iteration
in_sample_ses_by_iteration = c() #keep a growing list of se's by iteration
oos_ses_by_iteration = c() #keep a growing list of se's by iteration
i = 1

while (TRUE){

  #get all predictors left to try
  all_ses = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (!(j_try %in% predictor_by_iteration)){
      Xmm_sub = Xmm_train[, c(predictor_by_iteration, j_try), drop = FALSE]
      all_ses[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #lm.fit so much faster than lm! 
    }
  }
  j_star = which.min(all_ses)
  predictor_by_iteration = c(predictor_by_iteration, j_star)
  in_sample_ses_by_iteration = c(in_sample_ses_by_iteration, all_ses[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, predictor_by_iteration, drop = FALSE]
  mod = lm.fit(Xmm_sub, y_train)
  y_hat_test = Xmm_test[, predictor_by_iteration, drop = FALSE] %*% mod$coefficients
  oos_se = sd(y_test - y_hat_test)
  oos_ses_by_iteration = c(oos_ses_by_iteration, oos_se)
  
  cat("i = ", i, "in sample: se = ", all_ses[j_star], "oos_se", oos_se, "\n   predictor added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  predictor_by_iteration
  
  if (i > Nsamp || i > p_plus_one){
    break #why??
  }
  
}
```

Now let's look at the patterns

```{r}
simulation_results = data.frame(
  iteration = 1 : length(in_sample_ses_by_iteration),
  in_sample_ses_by_iteration = in_sample_ses_by_iteration,
  oos_ses_by_iteration = oos_ses_by_iteration
)

pacman::p_load(latex2exp)
ggplot(simulation_results) + 
  geom_line(aes(x = iteration, y = in_sample_ses_by_iteration), col = "red") +
  geom_line(aes(x = iteration, y = oos_ses_by_iteration), col = "blue") + 
  ylab(TeX("$s_e$"))
```


We can kind of see what the optimal model is above. If we want an exact procedure, we'd probably fit a separate smoothing regression to the oos results and analytically find the arg-minimum, $j^*$. That number will then be fed into the model matrix to create the right feature set and the final model will be produced with all the data.

Note that we will need a third split to honestly assess future performance as the test set above became our selection set because I was being sloppy.




