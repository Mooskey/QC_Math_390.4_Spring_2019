---
title: "Practice Lecture 7 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "March 19, 2019"
---


# Overfitting

Let's generate a linear model ($f = h^* \in \mathcal{H}$) and let $\epsilon$ be random noise (the error due to ignorance) for $\mathbb{D}$ featuring $n = 2$.

This simulation is random, but to ensure it looks the same to me right now as it does in class (and to you at home), let's "set the seed" so it is deterministic.

```{r}
set.seed(1003)
pacman::p_load(ggplot2)
```

Now let's "randomly generate" the data:

```{r}
n = 2
beta_0 = 1
beta_1 = 1
x = rnorm(n)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon

#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y))+
  xlim(-4, 4) + ylim(-5, 5) +
  geom_point()
basic
```

Let's now fit a linear model to this and plot:

```{r}
mod = lm(y ~ x)
b_0 = coef(mod)[1]
b_1 = coef(mod)[2]
basic + geom_abline(intercept = b_0, slope = b_1, col = "grey")
```

Note that obviously:

```{r}
summary(mod)$r.squared
summary(mod)$sigma #RMSE - technically cannot divide by zero so cannot estimate the RMSE - I made a slight technical error last class saying RMSE -> 0. I should've said s_e -> 0
sd(mod$residuals) #s_e
```

And let's plot the true function $h^*$ below as well in green:

```{r}
basic_and_lines = basic + 
  geom_abline(intercept = b_0, slope = b_1, col = "grey") +
  geom_abline(intercept = beta_0, slope = beta_1, col = "green") + 
  geom_segment(aes(x = x, y = h_star_x, xend = x, yend = y), col = "red")
basic_and_lines
```

The red lines are the epsilons:

```{r}
epsilon
```

Now let's envision some new data not in $\mathbb{D}$. We will call this "out of sample" (oos) since $\mathbb{D}$ defined our "sample" we used to build the model. For the oos data, we predict on it using our linear model $g$ which is far from the best linear model $h^*$ and we look at its residuals $e$:

```{r}
n_new = 50
x_new = rnorm(n_new)
h_star_x_new = beta_0 + beta_1 * x_new
epsilon_new = rnorm(n_new)
y_new = h_star_x_new + epsilon_new
y_hat_new = b_0 + b_1 * x_new

df_new = data.frame(x = x_new, y = y_new, h_star_x = h_star_x_new, y_hat = y_hat_new, e = y - y_hat_new)

basic_and_lines + 
  geom_point(data = df_new) + 
  geom_segment(data = df_new, aes(x = x, y = y_hat_new, xend = x, yend = y), col = "purple")
```

Instead of the residuals let's look at its true errors, epsilon, the distance from $h^*$:

```{r}
basic_and_lines + 
  geom_point(data = df_new) + 
  geom_segment(data = df_new, aes(x = x, y = h_star_x, xend = x, yend = y), col = "darkgrey")
```

The errors that the overfit model are worse than the errors made by the best model. In other words, the residual standard error on the new "out of sample" data is larger than the actual epsilon standard error:

```{r}
sd(df_new$e)
sd(epsilon_new)
```

How did we get in this mess? We can see from the picture we are using the grey line as $g$ but we should be using the green line $h^*$. We are using the grey line because we used the original $\epsilon$ to fit. BAD idea - won't generalize to the future.

```{r}
rm(list = ls())
```

The problem is bad for $n = 2$. But is it always bad? Let's take a look at $n = 100$ with a strong linear relationship with one predictor. 

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
x = runif(n, xmin, xmax)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

The true relationship is plotted below in green.

```{r}
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point() +
  geom_abline(intercept = beta_0, slope = beta_1, col = "darkgreen")
basic
```

And the estimated line $g$ (in blue) is pretty close:

```{r}
mod = lm(y ~ x)
b = coef(mod)
basic +
  geom_abline(intercept = b[1], slope = b[2], col = "blue")
```

They're basically right on top of each other: estimation error near zero.

```{r}
b
c(beta_0, beta_1)
```

And the $R^2$ is:

```{r}
summary(mod)$r.squared
```

Not great... plenty of room for overfitting the nonsense epsilons.

Now what happens if we add a random predictor? 

* We know that $R^2$ will go up. 
* But since this predictor is random, it is independent of the best model $h^*(x)$, hence it will constitute *overfitting*. 
* Overfitting is bad because it induces estimation error and $g$ will diverge from $h*$ (previous demo)
* This divergence leads to bad oos error (generalization error) meaning subpar predictions when we actually use the model in the future

Okay - but how bad? It only depends on how much $g$ diverges from $h^*$. Let's look at this divergence slowly. We create a new oos data set made from evenly spaced $x$ values across its range and random values of the nonsense predictors. We use this to calculated oos $s_e$.

```{r}
p_fake = 96
X = data.frame(matrix(c(x, rnorm(n * p_fake)), ncol = 1 + p_fake, nrow = n))
mod = lm(y ~ ., X)
y_hat = predict(mod, X)
s_e_in_sample = sd(y - y_hat)

nstar = 300
Xstar = matrix(c(seq(xmin, xmax, length.out = nstar), rnorm(nstar * p_fake)), ncol = 1 + p_fake)
y_stars = beta_0 + beta_1 * Xstar[, 1] + rnorm(nstar)
y_hat_stars = cbind(1, Xstar) %*% as.matrix(coef(mod))
s_e_oos = sd(y_stars - y_hat_stars)
basic_with_yhat = basic +
  geom_point(data = data.frame(x = x, y = y_hat), aes(x = x, y = y), col = "purple") + 
  xlim(0, 1) + ylim(-3, 8) +
  ggtitle(
    paste("Linear Model with", p_fake, "fake predictors"), 
    paste("Rsq =", round(summary(mod)$r.squared * 100, 2), "percent, in-sample s_e =", round(s_e_in_sample, 2), "and oos s_e =", round(s_e_oos, 2)))
basic_with_yhat

basic_with_yhat +
  geom_line(data = data.frame(x = Xstar[, 1], y = y_hat_stars), aes(x = x, y = y), col = "orange")
```

Lesson: it takes a bit of time to overfit badly. Don't worry about a few extra degrees of freedom if you have $n$ much larger than $p$. But it will eventually be corrosive!


# Assessing overfitting in practice

Let's examine this again. This time we use one data set which is split between training and testing.

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
p = 50
X = matrix(runif(n * p, xmin, xmax), ncol = p)

#best possible model - only one predictor matters!
h_star_x = beta_0 + beta_1 * X[,1 ]

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

Now we split $\mathbb{D}$ into training and testing. We define $K$ first, the inverse proportion of the test size.

```{r}
K = 5 #i.e. the test set is 1/5th of the entire historical dataset

#a simple algorithm to do this is to sample indices directly
test_indices = sample(1 : n, 1 / K * n)
train_indices = setdiff(1 : n, test_indices)

#now pull out the matrices and vectors based on the indices
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]

#let's ensure these are all correct
dim(X_train)
dim(X_test)
length(y_train)
length(y_test)
```

Now let's fit the model $g$ to the training data and compute in-sample error metrics:

```{r}
mod = lm(y_train ~ ., data.frame(X_train))
summary(mod)$r.squared
sd(mod$residuals)
```

Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:

```{r}
y_hat_oos = predict(mod, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2)
sd(oos_residuals)
```

MUCH worse!! Why? We overfit big time...

Can we go back now and fit a new model and see how we did? NO...

So how are we supposed to fix a "bad" model? We can't unless we do something smarter. We'll get there.

