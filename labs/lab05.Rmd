---
title: "Lab 5"
author: "Moshe Weiss"
output: pdf_document
date: "11:59PM March 16, 2019"
---

Load the Boston housing data frame and create the vector $y$ (the median value) and matrix $X$ (all other features) from the data frame. Name the columns the same as Boston except for the first name it "(Intercept)".

```{r}
data(Boston, package = "MASS")
y = Boston$medv
X = as.matrix(cbind(1, Boston[, 1 : 13]))
colnames(X)[1] = "(Intercept)"
```

Run the OLS linear model to get $b$, the vector of coefficients. Do not use `lm`.

```{r}
b = solve(t(X) %*% X) %*% t(X) %*% y
```

Find the hat matrix for this regression `H` and find its rank. Is this rank expected?

```{r}
H = X %*% solve(t(X) %*% X) %*% t(X)
dim(H)
pacman::p_load(Matrix)
rankMatrix(H)
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library's `expect_equal(matrix1, matrix2, tolerance = 1e-2)`.

```{r}
pacman::p_load(testthat)
expect_equal(H, t(H), tolerance = 1e-2)
expect_equal(H %*% H, H, tolerance = 1e-2)
```

Find the matrix that projects onto the space of residuals `H_comp` and find its rank. Is this rank expected?

```{r}
I = diag(nrow(H))
H_comp = (I - H)
rankMatrix(H_comp)
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library.

```{r}
expect_equal(H_comp %*% H_comp, H_comp, tolerance = 1e-2)
```

Calculate $\hat{y}$.

```{r}
yhat = H %*% y
head(yhat)
```

Calculate $e$ as the difference of $y$ and $\hat{y}$ and the projection onto the space of the residuals. Verify the two means of calculating the residuals provide the same results.

```{r}
e = y - yhat
e_2 = H_comp %*% y
expect_equal(e, e_2)
```

Calculate $R^2$ and RMSE.

```{r}
sse = sum(e^2)
sst = sum((y - mean(y))^2)

Rsquared = 1 - sse / sst
Rsquared

mse = sse / (nrow(X) - ncol(X))
rmse = sqrt(mse) #rmse is standard deviation of errors
rmse
```

Verify $\hat{y}$ and $e$ are orthogonal.

```{r}
t(e) %*% yhat
```

Verify $\hat{y} - \bar{y}$ and $e$ are orthogonal.

```{r}
t(e) %*% (yhat - mean(y))
```

Find the cosine-squared of $y - \bar{y}$ and $\hat{y} - \bar{y}$ and verify it is the same as $R^2$.

```{r}
y_minus_y_bar = y - mean(y)
yhat_minus_y_bar = yhat - mean(y)
len_y_minus_y_bar = sqrt(  sum(y_minus_y_bar^2)  )
len_yhat_minus_y_bar = sqrt(  sum(yhat_minus_y_bar^2)  )

theta = acos( (t(y_minus_y_bar) %*% yhat_minus_y_bar) / (len_y_minus_y_bar * len_yhat_minus_y_bar) )
theta * (180 / pi)
cos_theta_sqrd = cos(theta)^2
cos_theta_sqrd
```

Verify the sum of squares identity which we learned was due to the Pythagorean Theorem (applies since the projection is specifically orthogonal).

```{r}
len_y_minus_y_bar^2 - len_yhat_minus_y_bar^2 - sse 
```

Create a matrix that is $(p + 1) \times (p + 1)$ full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the $y$ regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the $y$ regressed on the first and second columns of $X$ only and put them in the first and second entries. For the third row, find the OLS estimates of the $y$ regressed on the first, second and third columns of $X$ only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
M = matrix(NA,ncol(X),ncol(X))

colnames(M)=colnames(X)
for(j in 1:ncol(M)){
  X_j=X[ , 1:j, drop = FALSE]
  b = solve(t(X_j) %*% X_j)%*%t(X_j)%*%y
  M[j, 1:j] = b
}

round(M, 2)
```

Examine this matrix. Why are the estimates changing from row to row as you add in more predictors?

Because each time, we're isolating more features and finding their variation from the mean

Clear the workspace and load the diamonds dataset.

```{r}
rm(list=ls())
pacman::p_load(ggplot2)
data(diamonds)
```

Extract $y$, the price variable and "c", the nominal variable "color" as vectors.

```{r}
summary(diamonds)
y = diamonds$price
c = diamonds$color
table(c)
```

Convert the "c" vector to $X$ which contains an intercept and an appropriate number of dummies. Let the color G be the refernce category as it is the modal color. Name the columns of $X$ appropriately. The first should be "(Intercept)". Delete c.

```{r}
X = rep(1,nrow(diamonds))
for(level in levels(diamonds$color)){
  if(level!="G"){X = cbind(X, diamonds$color == level)}
}
colnames(X) = c("Intercept", "Is_D", "Is_E","Is_F","Is_H","Is_I","Is_J")
head(X)
```

Repeat the iterative exercise above we did for Boston here.

```{r}
M = matrix(NA,ncol(X),ncol(X))

colnames(M)=colnames(X)
for(j in 1:ncol(M)){
  X_j=X[ , 1:j, drop = FALSE]
  b = solve(t(X_j) %*% X_j)%*%t(X_j)%*%y
  M[j, 1:j] = b
}

M

```

Why didn't the estimates change as we added more and more features?
  They did, for the same reason as before

Create a vector $y$ by simulating $n = 100$ standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the $R^2$ of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
y = rnorm(100)
X = matrix(cbind(rep(1,100),rnorm(100)),100,2)

b = solve(t(X) %*% X)%*%t(X)%*%y
H = X%*%solve(t(X) %*% X)%*%t(X)
yhat = X%*%b

e = y-yhat
sse = sum(e^2)
sst = sum((y-mean(y))^2)
Rsquared = 1- sse/sst
Rsquared



```

From the last problem. Find the $R^2$ of an OLS regression of `y ~ X`. You can use the `summary` function of an `lm` model.

```{r}
linear_model = lm('y~X')
summary(linear_model)$r.squared
summary(linear_model)$sigma
```

Write a for loop to each time bind a new column of 100 standard iid normals to the matrix $X$ and find the $R^2$ each time until the number of columns is 100. Create a vector to save all $R^2$'s. What happened??


```{r}
numRows = nrow(X)
startCol = ncol(X) + 1
r_squared_vec = c()
X = as.data.frame(X)
y=as.data.frame(y)
for(j in (startCol : 100)){
  X[,j] = rnorm(numRows,0,1)
  r_squared_vec = c(r_squared_vec, summary(lm('as.matrix(y)~as.matrix(X)'))$r.squared)
}
r_squared_vec

#Everytime we add a new independent column into our 'historical data' matrix X, our R^2 goes up, regardless of the meaningfulness of that data. When we've a matrix of linearly independent columns that is nxn with full rank, our matrix becomes invertible and so the projection of y onto y hat is y.
```

Add one final column to $X$ to bring the number of columns to 101. Then try to compute $R^2$. What happens and why?

```{r}

X[,ncol(X)+1] = rnorm(1,0,1)
summary(lm('as.matrix(y)~as.matrix(X)'))$r.squared

#R squared remains one because our matrix X when dim(X) = n x n spans n dimensional space. Additional columns thereafter are linear combinations of the first n, and add no new information.
```

